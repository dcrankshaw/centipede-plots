{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Straggler experiments (figure 9)\n",
    "\n",
    "Use bagging to train an ensemble of models for Clipper to demonstrate the effects of stragglers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.linear_model\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Changing the shape of non-C contiguous array')\n",
    "\n",
    "\n",
    "def load_digits(digits_location, digits_filename = \"train-mnist-dense-with-labels.data\"):\n",
    "    digits_path = os.path.join(digits_location, digits_filename)\n",
    "    print \"Source file:\", digits_path\n",
    "    df = pd.read_csv(digits_path, sep=\",\", header=None)\n",
    "    data = df.values\n",
    "    print \"Number of image files:\", len(data)\n",
    "    y = data[:,0]\n",
    "    X = data[:,1:]\n",
    "    return (X, y)\n",
    "\n",
    "def to_image(x):\n",
    "    return np.reshape(x,[28,28])\n",
    "\n",
    "def display_digit(x):\n",
    "    plt.imshow(to_image(x), interpolation='none')\n",
    "\n",
    "def display_random_digits(X, y):\n",
    "    ind = np.random.permutation(len(X))\n",
    "    plt.figure()\n",
    "    for i in range(0, 16):\n",
    "        plt.subplot(4,4,i+1)\n",
    "        display_digit(X[ind[i],:])\n",
    "        plt.draw()\n",
    "        # Display the plot\n",
    "\n",
    "        \n",
    "def normalize_digits(X):\n",
    "    mu = np.mean(X,0)\n",
    "    sigma = np.var(X,0)\n",
    "    Z = (X - mu) / np.array([np.sqrt(z) if z > 0 else 1. for z in sigma])\n",
    "    return Z \n",
    "\n",
    "def fourier_project(X, nfeatures = 4096, scale = 1.e-4):\n",
    "    (n,d) = X.shape\n",
    "    W = np.random.normal(scale = scale, size = [d, nfeatures])\n",
    "    phase = np.random.uniform( size = [1, nfeatures]) * 2.0 * np.pi\n",
    "    randomFeatures = np.cos(X.dot(W) + phase)\n",
    "    return randomFeatures\n",
    "\n",
    "def filter_two_class(X, y, digitA = 3, digitB = 9):\n",
    "    indexes = (y == (digitA + 1)) | (y == (digitB + 1))\n",
    "    binary_labels = (y == (digitA + 1)) * 1.\n",
    "    return (X[indexes], binary_labels[indexes])\n",
    "    \n",
    "#     return (yInd, yBinary[yInd])\n",
    "\n",
    "\n",
    "def train_test_split(y, propTrain = 0.75):\n",
    "    ind = np.random.permutation(len(y))\n",
    "    split_ind = ind[0.75 * len(y)]\n",
    "    train_ind = ind[:split_ind]\n",
    "    test_ind = ind[split_ind:]\n",
    "    print \"Train size: \", len(train_ind)\n",
    "    print \"Train true: \", np.mean(y[train_ind] == 1.0)\n",
    "    print \"Test size:  \", len(test_ind)\n",
    "    print \"Test true:  \", np.mean(y[test_ind] == 1.0)\n",
    "    return (train_ind, test_ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source file: /Users/crankshaw/model-serving/data/mnist_data/train-mnist-dense-with-labels.data\n",
      "Number of image files: 60000\n",
      "Source file: /Users/crankshaw/model-serving/data/mnist_data/test-mnist-dense-with-labels.data\n",
      "Number of image files: 10000\n"
     ]
    }
   ],
   "source": [
    "# # Load data\n",
    "train_x, train_y = load_digits(os.path.expanduser(\"/Users/crankshaw/model-serving/data/mnist_data\"))\n",
    "train_x = normalize_digits(train_x)\n",
    "\n",
    "test_x, test_y = load_digits(os.path.expanduser(\"/Users/crankshaw/model-serving/data/mnist_data\"), \"test-mnist-dense-with-labels.data\")\n",
    "test_x = normalize_digits(test_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def run_trial():\n",
    "    ensemble_size=16\n",
    "    # Train decision tree classifiers\n",
    "    models = BaggingClassifier(base_estimator=RandomForestClassifier(n_estimators=5), n_estimators=ensemble_size, max_samples=0.05, n_jobs=-1)\n",
    "    models.fit(train_x, train_y)\n",
    "\n",
    "    # Look at baseline score\n",
    "    print \"Ensemble score: %f\" % models.score(test_x, test_y)\n",
    "    print \"individual scores\"\n",
    "    for m in models.estimators_:\n",
    "        print m.score(test_x, test_y - 1)\n",
    "\n",
    "    # Run straggler experiment\n",
    "\n",
    "    def vote(estimators, x, y):\n",
    "        votes = np.zeros(len(estimators[0].classes_))\n",
    "        for m in estimators:\n",
    "            y_pred = m.predict(x)\n",
    "            votes[y_pred] += 1\n",
    "        y_hat = np.argmax(votes) + 1\n",
    "        return int(y_hat == y)\n",
    "\n",
    "    print \"cumulative score\"\n",
    "    scores = []    \n",
    "    for num_estimators in range(1,ensemble_size+1):\n",
    "        num_correct = 0\n",
    "        for i in range(len(test_y)):\n",
    "            num_correct += vote(models.estimators_[:num_estimators], test_x[i].reshape(1,-1), test_y[i])\n",
    "        score = float(num_correct)/float(len(test_y))\n",
    "        print score\n",
    "        scores.append(score)\n",
    "    scores.reverse()\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRIAL 0\n",
      "Ensemble score: 0.929000\n",
      "individual scores\n",
      "0.8173\n",
      "0.7989\n",
      "0.8031\n",
      "0.78\n",
      "0.6382\n",
      "0.7697\n",
      "0.7838\n",
      "0.7868\n",
      "0.7224\n",
      "0.8026\n",
      "0.7941\n",
      "0.796\n",
      "0.7967\n",
      "0.7756\n",
      "0.7815\n",
      "0.8114\n",
      "cumulative score\n",
      "0.8173\n",
      "0.7788\n",
      "0.8467\n",
      "0.8656\n",
      "0.8708\n",
      "0.8787\n",
      "0.8863\n",
      "0.8965\n",
      "0.8976\n",
      "0.9014\n",
      "0.9051\n",
      "0.9082\n",
      "0.9094\n",
      "0.9113\n",
      "0.9117\n",
      "0.9146\n",
      "TRIAL 1\n",
      "Ensemble score: 0.931000\n",
      "individual scores\n",
      "0.8005\n",
      "0.81\n",
      "0.8144\n",
      "0.7858\n",
      "0.7992\n",
      "0.8069\n",
      "0.8082\n",
      "0.7802\n",
      "0.8063\n",
      "0.7971\n",
      "0.7598\n",
      "0.7898\n",
      "0.8079\n",
      "0.7941\n",
      "0.8109\n",
      "0.7676\n",
      "cumulative score\n",
      "0.8005\n",
      "0.7805\n",
      "0.8451\n",
      "0.8656\n",
      "0.881\n",
      "0.8877\n",
      "0.8916\n",
      "0.8992\n",
      "0.9028\n",
      "0.9062\n",
      "0.9067\n",
      "0.9084\n",
      "0.9109\n",
      "0.9128\n",
      "0.9159\n",
      "0.9165\n",
      "TRIAL 2\n",
      "Ensemble score: 0.930400\n",
      "individual scores\n",
      "0.7984\n",
      "0.7868\n",
      "0.76\n",
      "0.7972\n",
      "0.8115\n",
      "0.7904\n",
      "0.7802\n",
      "0.7991\n",
      "0.808\n",
      "0.8067\n",
      "0.8033\n",
      "0.7909\n",
      "0.7142\n",
      "0.7996\n",
      "0.7816\n",
      "0.767\n",
      "cumulative score\n",
      "0.7984\n",
      "0.7524\n",
      "0.8255\n",
      "0.856\n",
      "0.8748\n",
      "0.8843\n",
      "0.8904\n",
      "0.8961\n",
      "0.9008\n",
      "0.9043\n",
      "0.907\n",
      "0.9124\n",
      "0.913\n",
      "0.9134\n",
      "0.9121\n",
      "0.9133\n",
      "TRIAL 3\n",
      "Ensemble score: 0.927800\n",
      "individual scores\n",
      "0.7838\n",
      "0.8003\n",
      "0.7596\n",
      "0.8087\n",
      "0.807\n",
      "0.7924\n",
      "0.7943\n",
      "0.6913\n",
      "0.6982\n",
      "0.7679\n",
      "0.7765\n",
      "0.8006\n",
      "0.8115\n",
      "0.791\n",
      "0.8079\n",
      "0.7766\n",
      "cumulative score\n",
      "0.7838\n",
      "0.7595\n",
      "0.8274\n",
      "0.8588\n",
      "0.8755\n",
      "0.8847\n",
      "0.8916\n",
      "0.8922\n",
      "0.8947\n",
      "0.899\n",
      "0.9032\n",
      "0.9046\n",
      "0.9075\n",
      "0.9108\n",
      "0.9119\n",
      "0.9117\n",
      "TRIAL 4\n",
      "Ensemble score: 0.930700\n",
      "individual scores\n",
      "0.7572\n",
      "0.795\n",
      "0.8056\n",
      "0.8082\n",
      "0.7436\n",
      "0.7781\n",
      "0.7946\n",
      "0.7977\n",
      "0.7975\n",
      "0.7993\n",
      "0.7923\n",
      "0.7853\n",
      "0.7752\n",
      "0.8113\n",
      "0.8095\n",
      "0.814\n",
      "cumulative score\n",
      "0.7572\n",
      "0.7336\n",
      "0.8316\n",
      "0.8589\n",
      "0.8705\n",
      "0.8748\n",
      "0.8866\n",
      "0.8906\n",
      "0.8959\n",
      "0.8993\n",
      "0.9031\n",
      "0.9077\n",
      "0.9086\n",
      "0.9101\n",
      "0.9126\n",
      "0.9142\n",
      "TRIAL 5\n",
      "Ensemble score: 0.930200\n",
      "individual scores\n",
      "0.7649\n",
      "0.7954\n",
      "0.7467\n",
      "0.7596\n",
      "0.7811\n",
      "0.7808\n",
      "0.7942\n",
      "0.7946\n",
      "0.8084\n",
      "0.7926\n",
      "0.7575\n",
      "0.7859\n",
      "0.8026\n",
      "0.7996\n",
      "0.8077\n",
      "0.7765\n",
      "cumulative score\n",
      "0.7649\n",
      "0.7419\n",
      "0.819\n",
      "0.8457\n",
      "0.865\n",
      "0.8739\n",
      "0.8837\n",
      "0.8877\n",
      "0.8938\n",
      "0.8999\n",
      "0.8992\n",
      "0.9029\n",
      "0.9055\n",
      "0.9077\n",
      "0.9105\n",
      "0.9129\n"
     ]
    }
   ],
   "source": [
    "trials = []\n",
    "for t in range(6):\n",
    "    print \"TRIAL %d\" % t\n",
    "    trials.append(run_trial())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.91386667  0.91245     0.91101667  0.90915     0.90736667  0.90405\n",
      "  0.90168333  0.8976      0.89371667  0.88836667  0.88068333  0.87293333\n",
      "  0.85843333  0.83255     0.75778333  0.78701667]\n",
      "[ 0.00150185  0.00166908  0.00186317  0.00239078  0.00302251  0.00264496\n",
      "  0.00268912  0.00324088  0.00390957  0.00300592  0.0052123   0.00497583\n",
      "  0.00672227  0.01015263  0.01744892  0.02088456]\n",
      "[[0.9146, 0.9117, 0.9113, 0.9094, 0.9082, 0.9051, 0.9014, 0.8976, 0.8965, 0.8863, 0.8787, 0.8708, 0.8656, 0.8467, 0.7788, 0.8173], [0.9165, 0.9159, 0.9128, 0.9109, 0.9084, 0.9067, 0.9062, 0.9028, 0.8992, 0.8916, 0.8877, 0.881, 0.8656, 0.8451, 0.7805, 0.8005], [0.9133, 0.9121, 0.9134, 0.913, 0.9124, 0.907, 0.9043, 0.9008, 0.8961, 0.8904, 0.8843, 0.8748, 0.856, 0.8255, 0.7524, 0.7984], [0.9117, 0.9119, 0.9108, 0.9075, 0.9046, 0.9032, 0.899, 0.8947, 0.8922, 0.8916, 0.8847, 0.8755, 0.8588, 0.8274, 0.7595, 0.7838], [0.9142, 0.9126, 0.9101, 0.9086, 0.9077, 0.9031, 0.8993, 0.8959, 0.8906, 0.8866, 0.8748, 0.8705, 0.8589, 0.8316, 0.7336, 0.7572], [0.9129, 0.9105, 0.9077, 0.9055, 0.9029, 0.8992, 0.8999, 0.8938, 0.8877, 0.8837, 0.8739, 0.865, 0.8457, 0.819, 0.7419, 0.7649]]\n"
     ]
    }
   ],
   "source": [
    "print np.mean(trials, axis=0)\n",
    "print np.std(trials, axis=0)\n",
    "print trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print scores\n",
    "scores.reverse()\n",
    "# scores = scores[:16]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(16), scores[:16])\n",
    "ax.set_xlabel(\"Size of ensemble\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_ylim((ax.get_ylim()[0],1.0))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "base_path = \"/Users/crankshaw/clipper-rust/model_wrappers/python/sklearn_models\"\n",
    "for (i,m) in enumerate(models.estimators_):\n",
    "    dname = os.path.join(base_path, \"sklearn_linsvm_10class_%d\" % (i+1))\n",
    "    os.mkdir(dname)\n",
    "    fname = os.path.join(dname, \"sklearn_linsvm_10class_%d.pkl\" % (i+1))\n",
    "    joblib.dump(m, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = models.estimators_[0].predict(test_x[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
