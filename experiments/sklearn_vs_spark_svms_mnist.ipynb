{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crankshaw/anaconda2/envs/clipper/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.svm as svm\n",
    "from sklearn.externals import joblib\n",
    "import os\n",
    "import sys\n",
    "from sample_feature import TestFeature\n",
    "import digits_functions as digits\n",
    "import findspark\n",
    "findspark.init(os.path.expanduser('~/model-serving/spark-1.6.0-bin-hadoop2.4'))\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionModel, LogisticRegressionWithSGD\n",
    "from pyspark.mllib.classification import SVMModel, SVMWithSGD\n",
    "# from pyspark.mllib.tree import RandomForestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "            .setAppName(\"crankshaw-pyspark\") \\\n",
    "            .set(\"spark.executor.memory\", \"2g\") \\\n",
    "            .set(\"spark.kryoserializer.buffer.mb\", \"128\") \\\n",
    "            .set(\"master\", \"local\")\n",
    "sc = SparkContext(conf=conf, batchSize=10)\n",
    "digits_base = \"/Users/crankshaw/model-serving/data/mnist_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_digits(digits_location, digits_filename = \"train.data\"):\n",
    "    digits_path = digits_location + \"/\" + digits_filename\n",
    "    print \"Source file:\", digits_path\n",
    "    df = pd.read_csv(digits_path, sep=\",\", header=None)\n",
    "    data = df.values\n",
    "    print \"Number of image files:\", len(data)\n",
    "    y = data[:,0]\n",
    "    X = data[:,1:]\n",
    "    return (X, y)\n",
    "\n",
    "def load_sk_models():\n",
    "    model_path = \"sklearn_models\"\n",
    "    features = []\n",
    "    for i in range(10):\n",
    "        p = \"%s/predict_%d_svm/predict_%d_svm.pkl\" % (model_path, i, i)\n",
    "        print \"loading %s\" % p\n",
    "        features.append(joblib.load(p))\n",
    "    return features\n",
    "\n",
    "# This code normalizes x as part of the prediction, so don't normalize the test data\n",
    "# ahead of time\n",
    "# def get_features(features, x):\n",
    "#     \"\"\"\n",
    "#     WORKS FOR SPARK FEATURES\n",
    "#     \"\"\"\n",
    "#     return np.array([f.predict(x.reshape(1,-1))[0] for f in features])\n",
    "\n",
    "\n",
    "\n",
    "def get_features(features, x):\n",
    "    \"\"\"\n",
    "    WORKS FOR SPARK FEATURES\n",
    "    \"\"\"\n",
    "    return np.array([float(f.predict(x)) for f in features])\n",
    "\n",
    "\n",
    "def load_spark_models():\n",
    "    \n",
    "    base_path = \"spark_models/svm_predict_%d\"\n",
    "    models = []\n",
    "    for i in range(1,11):\n",
    "        model = SVMModel.load(sc, base_path % i)\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "def train_tasks(tasks):\n",
    "    per_task_err = []\n",
    "    for i in range(len(tasks)):\n",
    "#         print 'task %d' % i\n",
    "        features_train = []\n",
    "        for j in range(tasks[i].X.shape[0]):\n",
    "            features_train.append(get_features(fs, tasks[i].X[j]))\n",
    "        features_train = np.array(features_train)\n",
    "        \n",
    "#         clf = lm.LogisticRegression(penalty=\"l1\", solver='liblinear', fit_intercept=False, C=1.0)\n",
    "        clf = lm.LogisticRegression(penalty=\"l2\", C=0.10, solver='liblinear')\n",
    "\n",
    "        clf.fit(features_train, tasks[i].y)\n",
    "        features_test=[]\n",
    "        for j in range(tasks[i].test_X.shape[0]):\n",
    "            features_test.append(get_features(fs, tasks[i].test_X[j]))\n",
    "        features_test = np.array(features_test)\n",
    "        predictions = clf.predict(features_test)\n",
    "        err = np.sum(predictions != tasks[i].test_y)\n",
    "        per_task_err.append(float(err)/float(len(predictions)))\n",
    "        if i % 10 == 0:\n",
    "            print np.mean(per_task_err)\n",
    "    return per_task_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source file: /Users/crankshaw/model-serving/data/mnist_data/test.data\n",
      "Number of image files: 10000\n",
      "1 [ 1.  0.  1.  1.  0.  1.  1.  0.  1.  0.]\n",
      "making task 0\n",
      "making task 50\n"
     ]
    }
   ],
   "source": [
    "test_x, test_y = load_digits(\"/Users/crankshaw/model-serving/data/mnist_data\", digits_filename=\"test.data\")\n",
    "# train_x, train_y = load_digits(\"/Users/crankshaw/model-serving/data/mnist_data\", \"train.data\")\n",
    "fs = load_spark_models()\n",
    "print test_y[10], get_features(fs, test_x[10])\n",
    "my_tasks = digits.create_mtl_datasets(test_x, test_y, nTasks=100, taskSize=30, testSize=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "0.161818181818\n",
      "0.173333333333\n",
      "0.176774193548\n",
      "0.18487804878\n",
      "0.186666666667\n",
      "0.182950819672\n",
      "0.183661971831\n",
      "0.193827160494\n",
      "0.190549450549\n",
      "[0.2, 0.22, 0.02, 0.18, 0.18, 0.04, 0.16, 0.16, 0.06, 0.28, 0.28, 0.22, 0.2, 0.04, 0.02, 0.16, 0.28, 0.16, 0.34, 0.2, 0.24, 0.02, 0.28, 0.16, 0.12, 0.08, 0.28, 0.26, 0.12, 0.2, 0.32, 0.2, 0.22, 0.22, 0.32, 0.32, 0.16, 0.3, 0.24, 0.1, 0.02, 0.18, 0.2, 0.26, 0.08, 0.26, 0.2, 0.28, 0.14, 0.0, 0.34, 0.08, 0.22, 0.2, 0.1, 0.2, 0.2, 0.06, 0.26, 0.14, 0.18, 0.22, 0.1, 0.18, 0.16, 0.3, 0.02, 0.3, 0.22, 0.12, 0.26, 0.42, 0.28, 0.28, 0.24, 0.34, 0.08, 0.36, 0.06, 0.3, 0.3, 0.12, 0.34, 0.22, 0.08, 0.06, 0.14, 0.16, 0.16, 0.22, 0.14, 0.18, 0.08, 0.22, 0.14, 0.04, 0.14, 0.12, 0.26, 0.24]\n",
      "0.1876\n"
     ]
    }
   ],
   "source": [
    "errs = train_tasks(my_tasks)\n",
    "print errs\n",
    "print np.mean(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.mean(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_spark_features(features, x):\n",
    "    return np.array([float(f.predict(x)) for f in features])\n",
    "\n",
    "\n",
    "def load_spark_models():\n",
    "    conf = SparkConf() \\\n",
    "            .setAppName(\"crankshaw-pyspark\") \\\n",
    "            .set(\"spark.executor.memory\", \"2g\") \\\n",
    "            .set(\"spark.kryoserializer.buffer.mb\", \"128\") \\\n",
    "            .set(\"master\", \"local\")\n",
    "    sc = SparkContext(conf=conf, batchSize=10)\n",
    "    base_path = \"spark_models/svm_predict_%d\"\n",
    "    models = []\n",
    "    for i in range(1,11):\n",
    "        model = SVMModel.load(sc, base_path % i)\n",
    "        models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Train Spark Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objective(x, pos_label):\n",
    "    # prediction objective\n",
    "    if x == pos_label:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    # return x\n",
    "\n",
    "def parseData(line, obj, pos_label):\n",
    "    fields = line.strip().split(',')\n",
    "    return LabeledPoint(obj(int(fields[0]), pos_label), [float(v)/255.0 for v in fields[1:]])\n",
    "\n",
    "def train_svm(pos_label):\n",
    "#     conf = SparkConf() \\\n",
    "#         .setAppName(\"crankshaw-pyspark\") \\\n",
    "#         .set(\"spark.executor.memory\", \"2g\") \\\n",
    "#         .set(\"spark.kryoserializer.buffer.mb\", \"128\") \\\n",
    "#         .set(\"master\", \"local\")\n",
    "#     sc = SparkContext(conf=conf, batchSize=10)\n",
    "    print 'Parsing data'\n",
    "    trainRDD = sc.textFile(\"/crankshaw-local/mnist/data/train_norm.data\").map(lambda line: parseData(line, objective, pos_label)).cache()\n",
    "    # testRDD = sc.textFile(\"/crankshaw-local/mnist/data/test.data\").map(lambda line: parseData(line, objective)).cache()\n",
    "\n",
    "    print 'Fitting model'\n",
    "\n",
    "    svm = SVMWithSGD.train(trainRDD)\n",
    "\n",
    "    path = 'spark_models/svm_predict_%d' % pos_label\n",
    "    svm.save(sc, path)\n",
    "    sc.stop()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
