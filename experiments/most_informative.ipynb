{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/crankshaw/anaconda2/envs/clipper/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.svm as svm\n",
    "from sklearn.externals import joblib\n",
    "import os\n",
    "import sys\n",
    "from sample_feature import TestFeature\n",
    "import digits_functions as digits\n",
    "import findspark\n",
    "findspark.init(os.path.expanduser('~/model-serving/spark-1.6.0-bin-hadoop2.4'))\n",
    "# findspark.init()\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionModel, LogisticRegressionWithSGD\n",
    "from pyspark.mllib.classification import SVMModel, SVMWithSGD\n",
    "# from pyspark.mllib.tree import RandomForestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def load_digits(digits_location, digits_filename = \"train.data\"):\n",
    "#     digits_path = digits_location + \"/\" + digits_filename\n",
    "#     print \"Source file:\", digits_path\n",
    "#     df = pd.read_csv(digits_path, sep=\",\", header=None)\n",
    "#     data = df.values\n",
    "#     print \"Number of image files:\", len(data)\n",
    "#     y = data[:,0]\n",
    "#     X = data[:,1:]\n",
    "#     return (X, y)\n",
    "\n",
    "# def load_models():\n",
    "#     model_path = \"sklearn_models\"\n",
    "#     features = []\n",
    "#     for i in range(10):\n",
    "#         p = \"%s/predict_%d_svm/predict_%d_svm.pkl\" % (model_path, i, i)\n",
    "#         print \"loading %s\" % p\n",
    "#         features.append(joblib.load(p))\n",
    "#     return features\n",
    "\n",
    "# # This code normalizes x as part of the prediction, so don't normalize the test data\n",
    "# # ahead of time\n",
    "# def get_features(features, x):\n",
    "#     return np.array([f.predict(x.reshape(1,-1))[0] for f in features])\n",
    "\n",
    "\n",
    "# def modify_data(X_train, X_test, ws, drop):\n",
    "#     dim = X_test.shape[1]-drop\n",
    "#     means = np.mean(X_train, axis=0)\n",
    "#     ind = np.argsort(abs(ws)).tolist()[0][::-1][0:dim]\n",
    "#     #ind = np.random.permutation(X_test.shape[1])[0:dim]\n",
    "#     features = np.zeros(X_test.shape)\n",
    "#     for i in range(X_test.shape[1]):\n",
    "#         if i in ind:\n",
    "#             features[:,i] = X_test[:,i]\n",
    "#         else:\n",
    "#             features[:,i] = means[i]\n",
    "#     return features\n",
    "\n",
    "# def drop_features(X_test, ws, drop):\n",
    "#     dim = X_test.shape[1]-drop\n",
    "#     ind = np.argsort(abs(ws)).tolist()[0][::-1][0:dim]\n",
    "#     #ind = np.random.permutation(X_test.shape[1])[0:dim]\n",
    "#     features = X_test[:,perm]\n",
    "#     return features\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def run(fs,tasks,method):\n",
    "#     dic_err = {}\n",
    "#     dic_num = {}\n",
    "#     for i in range(len(tasks)):\n",
    "#         print 'task %d:' % i\n",
    "#         print 'task preference: ', tasks[i].pref\n",
    "#         features_train = []\n",
    "#         for j in range(tasks[i].X.shape[0]):\n",
    "#             features_train.append(get_features(fs, tasks[i].X[j]))\n",
    "#         features_train = np.array(features_train)\n",
    "        \n",
    "#         clf = lm.LogisticRegression()\n",
    "#         clf.fit(features_train, tasks[i].y)\n",
    "#         if method == 'drop':\n",
    "#             features_train = drop_features(features_train, clf.coef_, drop)\n",
    "#         clf_new = lm.LogisticRegression()\n",
    "#         clf_new.fit(features_train, tasks[i].y)\n",
    "        \n",
    "#         #print clf_new.coef_\n",
    "# #         for drop in range(10):\n",
    "#         for drop in [0]:\n",
    "\n",
    "#             print 'drop %d features.' % drop\n",
    "#             features_test=[]\n",
    "#             for j in range(tasks[i].test_X.shape[0]):\n",
    "#                 features_test.append(get_features(fs, tasks[i].test_X[j]))\n",
    "#             features_test = np.array(features_test)\n",
    "#             if method == 'drop':\n",
    "#                 features_test = drop_features(features_test, clf.coef_, drop)\n",
    "#             else:\n",
    "#                 features_test = modify_data(features_train,features_test, clf.coef_, drop)\n",
    "\n",
    "#             pred = clf_new.predict(features_test)\n",
    "#             err = np.sum(pred != tasks[i].test_y)\n",
    "#             print 'error: ', err\n",
    "#             if drop in dic_err and drop in dic_num:\n",
    "#                 dic_err[drop] += err\n",
    "#                 dic_num[drop] += len(pred)\n",
    "#             else:\n",
    "#                 dic_err[drop] = err\n",
    "#                 dic_num[drop] = len(pred)\n",
    "    \n",
    "#     error_list = []\n",
    "#     for k in dic_err:\n",
    "#         error_list.append(dic_err[k]/float(dic_num[k]))\n",
    "#     #print 'average rate error: %f' % (np.mean(accuracy))\n",
    "#     return error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "            .setAppName(\"crankshaw-pyspark\") \\\n",
    "            .set(\"spark.executor.memory\", \"2g\") \\\n",
    "            .set(\"spark.kryoserializer.buffer.mb\", \"128\") \\\n",
    "            .set(\"master\", \"local\")\n",
    "sc = SparkContext(conf=conf, batchSize=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_digits(digits_location, digits_filename = \"train.data\"):\n",
    "    digits_path = digits_location + \"/\" + digits_filename\n",
    "    print \"Source file:\", digits_path\n",
    "    df = pd.read_csv(digits_path, sep=\",\", header=None)\n",
    "    data = df.values\n",
    "    print \"Number of image files:\", len(data)\n",
    "    y = data[:,0]\n",
    "    X = data[:,1:]\n",
    "    return (X, y)\n",
    "\n",
    "def load_sk_models():\n",
    "    model_path = \"sklearn_models\"\n",
    "    features = []\n",
    "    for i in range(10):\n",
    "        p = \"%s/predict_%d_svm/predict_%d_svm.pkl\" % (model_path, i, i)\n",
    "        print \"loading %s\" % p\n",
    "        features.append(joblib.load(p))\n",
    "    return features\n",
    "\n",
    "# This code normalizes x as part of the prediction, so don't normalize the test data\n",
    "# ahead of time\n",
    "# def get_features(features, x):\n",
    "#     \"\"\"\n",
    "#     WORKS FOR SPARK FEATURES\n",
    "#     \"\"\"\n",
    "#     return np.array([f.predict(x.reshape(1,-1))[0] for f in features])\n",
    "\n",
    "\n",
    "\n",
    "def get_features(features, x):\n",
    "    \"\"\"\n",
    "    WORKS FOR SPARK FEATURES\n",
    "    \"\"\"\n",
    "    return np.array([float(f.predict(x)) for f in features])\n",
    "\n",
    "\n",
    "def load_spark_models():\n",
    "    \n",
    "    base_path = \"spark_models/svm_predict_%d\"\n",
    "    models = []\n",
    "    for i in range(1,11):\n",
    "        model = SVMModel.load(sc, base_path % i)\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "def train_tasks(tasks):\n",
    "    per_task_err = []\n",
    "    for i in range(len(tasks)):\n",
    "#         print 'task %d' % i\n",
    "        features_train = []\n",
    "        for j in range(tasks[i].X.shape[0]):\n",
    "            features_train.append(get_features(fs, tasks[i].X[j]))\n",
    "        features_train = np.array(features_train)\n",
    "        \n",
    "#         clf = lm.LogisticRegression(penalty=\"l1\", solver='liblinear', fit_intercept=False, C=1.0)\n",
    "        clf = lm.LogisticRegression(penalty=\"l2\", C=0.10, solver='liblinear')\n",
    "\n",
    "        clf.fit(features_train, tasks[i].y)\n",
    "        features_test=[]\n",
    "        for j in range(tasks[i].test_X.shape[0]):\n",
    "            features_test.append(get_features(fs, tasks[i].test_X[j]))\n",
    "        features_test = np.array(features_test)\n",
    "        predictions = clf.predict(features_test)\n",
    "        err = np.sum(predictions != tasks[i].test_y)\n",
    "        per_task_err.append(float(err)/float(len(predictions)))\n",
    "        if i % 10 == 0:\n",
    "            print np.mean(per_task_err)\n",
    "    return per_task_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source file: /Users/crankshaw/model-serving/data/mnist_data/test.data\n",
      "Number of image files: 10000\n",
      "1 [ 1.  0.  1.  1.  0.  1.  1.  0.  1.  0.]\n",
      "making task 0\n",
      "making task 50\n"
     ]
    }
   ],
   "source": [
    "test_x, test_y = load_digits(\"/Users/crankshaw/model-serving/data/mnist_data\", digits_filename=\"test.data\")\n",
    "# train_x, train_y = load_digits(\"/Users/crankshaw/model-serving/data/mnist_data\", \"train.data\")\n",
    "fs = load_spark_models()\n",
    "print test_y[10], get_features(fs, test_x[10])\n",
    "my_tasks = digits.create_mtl_datasets(test_x, test_y, nTasks=100, taskSize=30, testSize=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "0.161818181818\n",
      "0.173333333333\n",
      "0.176774193548\n",
      "0.18487804878\n",
      "0.186666666667\n",
      "0.182950819672\n",
      "0.183661971831\n",
      "0.193827160494\n",
      "0.190549450549\n",
      "[0.2, 0.22, 0.02, 0.18, 0.18, 0.04, 0.16, 0.16, 0.06, 0.28, 0.28, 0.22, 0.2, 0.04, 0.02, 0.16, 0.28, 0.16, 0.34, 0.2, 0.24, 0.02, 0.28, 0.16, 0.12, 0.08, 0.28, 0.26, 0.12, 0.2, 0.32, 0.2, 0.22, 0.22, 0.32, 0.32, 0.16, 0.3, 0.24, 0.1, 0.02, 0.18, 0.2, 0.26, 0.08, 0.26, 0.2, 0.28, 0.14, 0.0, 0.34, 0.08, 0.22, 0.2, 0.1, 0.2, 0.2, 0.06, 0.26, 0.14, 0.18, 0.22, 0.1, 0.18, 0.16, 0.3, 0.02, 0.3, 0.22, 0.12, 0.26, 0.42, 0.28, 0.28, 0.24, 0.34, 0.08, 0.36, 0.06, 0.3, 0.3, 0.12, 0.34, 0.22, 0.08, 0.06, 0.14, 0.16, 0.16, 0.22, 0.14, 0.18, 0.08, 0.22, 0.14, 0.04, 0.14, 0.12, 0.26, 0.24]\n",
      "0.1876\n"
     ]
    }
   ],
   "source": [
    "errs = train_tasks(my_tasks)\n",
    "print errs\n",
    "print np.mean(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print np.mean(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_spark_features(features, x):\n",
    "    return np.array([float(f.predict(x)) for f in features])\n",
    "\n",
    "\n",
    "def load_spark_models():\n",
    "    conf = SparkConf() \\\n",
    "            .setAppName(\"crankshaw-pyspark\") \\\n",
    "            .set(\"spark.executor.memory\", \"2g\") \\\n",
    "            .set(\"spark.kryoserializer.buffer.mb\", \"128\") \\\n",
    "            .set(\"master\", \"local\")\n",
    "    sc = SparkContext(conf=conf, batchSize=10)\n",
    "    base_path = \"spark_models/svm_predict_%d\"\n",
    "    models = []\n",
    "    for i in range(1,11):\n",
    "        model = SVMModel.load(sc, base_path % i)\n",
    "        models.append(model)\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
